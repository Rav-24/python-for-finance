{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    " \n",
    "A support vector machine is a machine learning model that is able to generalise between two different classes if the set of labelled data is provided in the training set to the algorithm. The main function of the SVM is to check for that hyperplane that is able to distinguish between the two classes.\n",
    "\n",
    " \n",
    "There can be many hyperplanes that can do this task but the objective is to find that hyperplane that has the highest margin that means maximum distances between the two classes, so that in future if a new data point comes that is two be classified then it can be classified easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does SVM Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linearly Separable Data\n",
    "\n",
    "\n",
    "Let us understand the working of SVM by taking an example where we have two classes that are shown is the below image which are a class A: Circle & class B: Triangle. Now, we want to apply the SVM algorithm and find out the best hyperplane that divides the both classes.\n",
    "\n",
    "<img src=\"../data/SVM1.png\" alt=\"SVM1\" style=\"width: 450px;\"/>\n",
    "<img src=\"../data/SVM2.png\" alt=\"SVM2\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "SVM takes all the data points in consideration and gives out a line that is called ‘Hyperplane’ which divides both the classes. This line is termed as ‘Decision boundary’. Anything that falls in circle class will belong to the  class A and vice-versa.\n",
    "\n",
    "<img src=\"../data/SVM3.png\" alt=\"SVM3\" style=\"width: 500px;\"/>\n",
    "\n",
    "There can be many hyperplanes that you can see but the best hyper plane that divides the two classes would be the hyperplane having a large distance from the hyperplane from both the classes. That is the main motive of SVM to find such best hyperplanes.\n",
    "\n",
    "There can be different dimensions which solely depends upon the features we have. It is tough to visualize when the features are more than 3.\n",
    "\n",
    "<img src=\"../data/SVM4.png\" alt=\"SVM4\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Consider we have two classes that are red and yellow class A and B respectively. We need to find the best hyperplane between them that divides the two classes. \n",
    "\n",
    "<img src=\"../data/SVM5.png\" alt=\"SVM5\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Soft margin permits few of the above data points to get misclassified. Also,it tries to make the balance back and forth between finding a hyperplane that attempts to make less misclassifications and maximize the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linearly Non-separable Data\n",
    "\n",
    "<img src=\"../data/SVM6.png\" alt=\"SVM6\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If the data is non linearly separable as shown in the above figure then SVM makes use of kernel tricks to make it linearly separable. The concept of transformation of non-linearly separable data into linearly separable is called Cover’s theorem - “given a set of training data that is not linearly separable, with high probability it can be transformed into a linearly separable training set by projecting it into a higher-dimensional space via some non-linear transformation”. Kernel tricks help in projecting data points to the higher dimensional space by which they became relatively more easily separable in higher-dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Tricks: \n",
    "\n",
    "![SVM7](../data/SVM7.png)\n",
    "\n",
    "\n",
    "Kernel tricks also known as Generalized dot product. Kernel tricks are the way of calculating dot product of two vectors to check how much they make an effect on each other. According to Cover’s theorem the chances of linearly non-separable data sets becoming linearly separable increase in higher dimensions. Kernel functions are used to get the dot products to solve SVM constrained optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Kernel Functions:\n",
    " \n",
    "While using the svm classifier we can take the kernel as ‘linear’ , ’poly’ , ‘rbf’ , ‘sigmoid’. Let us see which are the most used kernels that are polynomial and rbf (Radial Basis Function). You can refer here for documentation that is present on sklearn.\n",
    "\n",
    "\n",
    "- Polynomial Kernel-  The process of generating  new features by using a polynomial combination of all the existing features.\n",
    "\n",
    "\n",
    "- Radial Basis Function(RBF) Kernel-  The process of generating new features calculating the distance between all other dots to a specific dot. One of the rbf kernels that is used widely is the Gaussian Radial Basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree of tolerance in SVM \n",
    " \n",
    "\n",
    "The penalty term that is passed as a hyper parameter in SVM while dealing with both linearly separable and non linear solutions is denoted as ‘C’ that is called as Degree of tolerance. Large value of C results in the more penalty SVM gets when it makes a misclassification. The decision boundary will be dependent on narrow margin and less support vectors.\n",
    "\n",
    " \n",
    "### Pros of SVM\n",
    "\n",
    "-    High stability due to dependency on support vectors and not the data points.\n",
    "\n",
    "-    Does not get influenced by Outliers. \n",
    "\n",
    "-    No assumptions made of the datasets.\n",
    "\n",
    "-    Numeric predictions problem can be dealt with SVM.\n",
    "\n",
    "\n",
    "### Cons of SVM\n",
    "\n",
    " \n",
    "\n",
    "- Blackbox method.\n",
    "\n",
    "- Inclined to overfitting method.\n",
    "\n",
    "- Very rigorous computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "44fd9b5e-4374-44c8-9c62-2b092a54c68b",
    "_uuid": "59be0dafdd352d41c1e7eb2db65c015cd1d67b12"
   },
   "source": [
    "**DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/05/mobileprice/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ca82ab6309ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/05/mobileprice/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/05/mobileprice/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/05/mobileprice/train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/05/mobileprice/train.csv')\n",
    "test = pd.read_csv('data/05/mobileprice/test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "88bb234a-9d85-4e29-8aa8-88e54345743a",
    "_uuid": "b380904bd662112cb8f9ba79c416633962c9d46f"
   },
   "outputs": [],
   "source": [
    "# checking if there is any missing value\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b002a54e-383b-4f76-9617-16722f4ae7b4",
    "_uuid": "19a148d76b45f5728572ef26f4d450397c99a2f5"
   },
   "source": [
    "**TARGET VALUE ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "460450c3-c004-4aa4-aa5d-df2286f70fe5",
    "_uuid": "5a35aaad3708055baa0f5961d405984e33f58ace"
   },
   "outputs": [],
   "source": [
    "#understanding the predicted value - which is hot encoded, in real life price won't be hot encoded.\n",
    "df['price_range'].describe(), df['price_range'].unique()\n",
    "\n",
    "# there are 4 classes in the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "39880eac-14e2-4ae3-95fd-cc59bb3be576",
    "_uuid": "cbc0da9a7d1407a89d7b1a626581cbe05f486ae4"
   },
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "f,ax = plt.subplots(figsize=(12,10))\n",
    "sns.heatmap(corrmat,vmax=0.8,square=True,annot=True,annot_kws={'size':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09309689-3aea-4b56-bcf5-ed1592acf867",
    "_uuid": "aebd8eaef413193325f162368a21dd33b882c83a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,4))\n",
    "plt.scatter(y=df['price_range'],x=df['battery_power'],color='red')\n",
    "plt.scatter(y=df['price_range'],x=df['ram'],color='Green')\n",
    "plt.scatter(y=df['price_range'],x=df['n_cores'],color='blue')\n",
    "plt.scatter(y=df['price_range'],x=df['mobile_wt'],color='orange')\n",
    "# clearly we can see that each of the category has different set of value ranges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aab929d8-5b3a-4a47-a0b0-b8d831078552",
    "_uuid": "0da42e6d68e5e50a22243df17a9797c9ddb28008"
   },
   "source": [
    "Now in the data set there is no need to create dummy variables or handle missing data as data set doesn't have any missing data \n",
    "\n",
    "**SUPPORT VECTOR MACHINES AND METHODS : **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd158f89-8437-4417-9c4c-f3bddc84a0d3",
    "_uuid": "7166482c61754c0ff3df01e6c6d0416bb1b2bfc6"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_t = np.array(df['price_range'])\n",
    "X_t = df\n",
    "X_t = df.drop(['price_range'],axis=1)\n",
    "X_t = np.array(X_t)\n",
    "\n",
    "print(\"shape of Y :\"+str(y_t.shape))\n",
    "print(\"shape of X :\"+str(X_t.shape))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_t = scaler.fit_transform(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d2117cb9-654e-4d8a-aa71-992156eeb6b1",
    "_uuid": "5be1d3ac05124b701ce0a4cd51cfaafe5da7731d"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_t,y_t,test_size=.20,random_state=42)\n",
    "print(\"shape of X Train :\"+str(X_train.shape))\n",
    "print(\"shape of X Test :\"+str(X_test.shape))\n",
    "print(\"shape of Y Train :\"+str(Y_train.shape))\n",
    "print(\"shape of Y Test :\"+str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a841504-52fe-4926-900f-b7a98160f215",
    "_uuid": "9f0c7c02180fc639afd8b166c0d36c3f3c237d48"
   },
   "outputs": [],
   "source": [
    "for this_C in [1,3,5,10,40,60,80,100]:\n",
    "    clf = SVC(kernel='linear',C=this_C).fit(X_train,Y_train)\n",
    "    scoretrain = clf.score(X_train,Y_train)\n",
    "    scoretest  = clf.score(X_test,Y_test)\n",
    "    print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,scoretrain,scoretest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0ced714f-0c0e-44d1-aef5-496b545de189",
    "_uuid": "74cefa3fcd0748c92d96cfab6f343f73496d15b1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,LeaveOneOut\n",
    "clf1 = SVC(kernel='linear',C=20).fit(X_train,Y_train)\n",
    "scores = cross_val_score(clf1,X_train,Y_train,cv=5)\n",
    "strat_scores = cross_val_score(clf1,X_train,Y_train,cv=StratifiedKFold(5,random_state=10,shuffle=True))\n",
    "#Loo = LeaveOneOut()\n",
    "#Loo_scores = cross_val_score(clf1,X_train,Y_train,cv=Loo)\n",
    "print(\"The Cross Validation Score :\"+str(scores))\n",
    "print(\"The Average Cross Validation Score :\"+str(scores.mean()))\n",
    "print(\"The Stratified Cross Validation Score :\"+str(strat_scores))\n",
    "print(\"The Average Stratified Cross Validation Score :\"+str(strat_scores.mean()))\n",
    "#print(\"The LeaveOneOut Cross Validation Score :\"+str(Loo_scores))\n",
    "#print(\"The Average LeaveOneOut Cross Validation Score :\"+str(Loo_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bedfd3a4a9849d79f0aebded7ba900f06188a59b"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "for strat in ['stratified', 'most_frequent', 'prior', 'uniform']:\n",
    "    dummy_maj = DummyClassifier(strategy=strat).fit(X_train,Y_train)\n",
    "    print(\"Train Stratergy :{} \\n Score :{:.2f}\".format(strat,dummy_maj.score(X_train,Y_train)))\n",
    "    print(\"Test Stratergy :{} \\n Score :{:.2f}\".format(strat,dummy_maj.score(X_test,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f769b170-b8c1-40b5-b2a4-e77147c51f3d",
    "_uuid": "258cde3bb984854f8efc776456704621546ba28a"
   },
   "outputs": [],
   "source": [
    "# plotting the decision boundries for the data \n",
    "#converting the data to array for plotting. \n",
    "X = np.array(df.iloc[:,[0,13]])\n",
    "y = np.array(df['price_range'])\n",
    "print(\"Shape of X:\"+str(X.shape))\n",
    "print(\"Shape of y:\"+str(y.shape))\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb52356e-54f7-440a-8a93-493ddc6d9828",
    "_uuid": "e30d5991bb9ec84737df5db10b55082a52150b0a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# custome color maps\n",
    "cm_dark = ListedColormap(['#ff6060', '#8282ff','#ffaa00','#fff244','#4df9b9','#76e8fc','#3ad628'])\n",
    "cm_bright = ListedColormap(['#ffafaf', '#c6c6ff','#ffaa00','#ffe2a8','#bfffe7','#c9f7ff','#9eff93'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f9d9ffb-01a0-43fc-b133-1bd4b7aee27a",
    "_uuid": "249279d2398462aefadbbc1fd5bf186c8f8bd110"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=y,cmap=cm_dark,s=10,label=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "822fa316-06b0-4207-8bc8-348713a62b92",
    "_uuid": "c331b468bf4898754fd9adf9231df2bbb086bf86"
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "C_param = 1 # No of neighbours\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf1 = SVC(kernel='linear',C=C_param)\n",
    "    clf1.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min()-.20, X[:, 0].max()+.20\n",
    "    y_min, y_max = X[:, 1].min()-.20, X[:, 1].max()+.20\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf1.predict(np.c_[xx.ravel(), yy.ravel()])   # ravel to flatten the into 1D and c_ to concatenate \n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cm_bright)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_dark,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"SVM Linear Classification (kernal = linear, Gamma = '%s')\"% (C_param))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29b5c886-cedc-4791-8dc0-b2da8f7febd0",
    "_uuid": "cea93036779601331a4e8e009b5e2e6116e603f3"
   },
   "outputs": [],
   "source": [
    "print(\"The score of the above :\"+str(clf1.score(X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f26fe33c-fbac-4527-9ba3-8896d7621821",
    "_uuid": "30f878663a0ac5fc43adbd7f598370f8ad3049ba"
   },
   "outputs": [],
   "source": [
    "# Linear Support vector machine with only C Parameter \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "for this_C in [1,3,5,10,40,60,80,100]:\n",
    "    clf2 = LinearSVC(C=this_C).fit(X_train,Y_train)\n",
    "    scoretrain = clf2.score(X_train,Y_train)\n",
    "    scoretest  = clf2.score(X_test,Y_test)\n",
    "    print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,scoretrain,scoretest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83f1b6d3-5f89-48c3-8375-3e3d9dfe106f",
    "_uuid": "c7164190b883c32dbe1a7530e403ad6410aad53a"
   },
   "source": [
    "Apparently we got better scores with SVC where we defined the kernal as linear than with just LinearSVC\n",
    "\n",
    "The LinearSVC class is based on the liblinear library, which implements an optimized algorithm for linear SVMs.\n",
    "1. It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features: its training time complexity is roughly O(m × n).\n",
    "\n",
    "The SVC class is based on the libsvm library, which implements an algorithm that supports the kernel trick.\n",
    "1. The training time complexity is usually between O(m2 × n) and O(m3 × n). \n",
    "1. LinearSVC is much faster than SVC(kernel=\"linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3978887a-e22b-4ce8-8e6f-64d47591aaf9",
    "_uuid": "812ac486811dd7fc2116d04fb053eb8c3fb09ee5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='linear',C=1,epsilon=.01).fit(X_train,Y_train)\n",
    "print(\"{:.2f} is the accuracy of the SV Regressor\".format(svr.score(X_train,Y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1e04aaa-9fbd-41ec-a15e-0e1f9cc67163",
    "_uuid": "824f056757dad9ac79ac567a0ec284f2edcef122"
   },
   "source": [
    "* SVM supports linear and nonlinear regression.\n",
    "* SVM Regression tries to fit as many instances as possible on the decision boundary while limiting margin violations.\n",
    "* The width of the decision boundary is controlled by a hyperparameter ϵ. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23da8689-2020-4e30-9990-0a7a69646a7e",
    "_uuid": "219bc19e91cf8fde8bfdefb2ce47ffacadded2ac"
   },
   "source": [
    "**NON LINEAR SVM**\n",
    "\n",
    "A method to Handle Non linear relationships in our data set is to use polynomial Kernal or using a similarity function with our SVM.\n",
    "\n",
    "We will use the Gaussian Radial Basis Function(RBF) function for the same. to handle this in Sklearn there is a Gamma hyperparameter. \n",
    "Check the Gausian RBF Function - for more info. \n",
    "\n",
    "Technically, the gamma parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points. Intuitively, a small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. **In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other.**\n",
    "\n",
    "Initution : we create different landmarks and then check how far the training examples are from the landmark. In practise, if we have n training examples then we will have n landmarks and we will thus create a feature set of n values with n landmarks. When the training example is closest to a landmark the value the variance will be small and when far the value will be large and hence we will associate the close to the landmark example with a 1 and those that are far with a 0. This ability makes the SVM very powerful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e6ee5f0-ebad-40d5-a3f4-8bb034e2e069",
    "_uuid": "4f00573c1050ffa1c827277fab4df5663c8b50af"
   },
   "outputs": [],
   "source": [
    "# SMV with RBF KERNAL AND ONLY C PARAMETER \n",
    "\n",
    "for this_C in [1,5,10,25,50,100]:\n",
    "    clf3 = SVC(kernel='rbf',C=this_C).fit(X_train,Y_train)\n",
    "    clf3train = clf3.score(X_train,Y_train)\n",
    "    clf3test  = clf3.score(X_test,Y_test)\n",
    "    print(\"SVM for Non Linear \\n C:{} Training Score : {:2f} Test Score : {:2f}\\n\".format(this_C,clf3train,clf3test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "785e7823-68ea-457b-aa0f-cdcc63009e7a",
    "_uuid": "8cb1bbc811ddfb888d2713d457e7819ec8b95259"
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "C_param = 1 # No of neighbours\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf1 = SVC(kernel='rbf',C=C_param)\n",
    "    clf1.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf1.predict(np.c_[xx.ravel(), yy.ravel()])   # ravel to flatten the into 1D and c_ to concatenate \n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cm_bright)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_dark,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"SVM Linear Classification (kernal = linear, Gamma = '%s')\"% (C_param))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65c30a5e-ff26-450b-8bcf-b2e3e7facc1b",
    "_uuid": "7d5e6aebafc97cbbac2dd2c999a73a94a5a37140"
   },
   "outputs": [],
   "source": [
    "# SVM WITH RBF KERNAL, C AND GAMMA HYPERPARAMTER \n",
    "for this_gamma in [.1,.5,.10,.25,.50,1]:\n",
    "    for this_C in [1,5,7,10,15,25,50]:\n",
    "        clf3 = SVC(kernel='rbf',C=this_C,gamma=this_gamma).fit(X_train,Y_train)\n",
    "        clf3train = clf3.score(X_train,Y_train)\n",
    "        clf3test  = clf3.score(X_test,Y_test)\n",
    "        print(\"SVM for Non Linear \\n Gamma: {} C:{} Training Score : {:2f} Test Score : {:2f}\\n\".format(this_gamma,this_C,clf3train,clf3test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1bdc1023-2463-4061-838d-d88b24c9abef",
    "_uuid": "5d7eac401e043fb97096220d268b7f64cd15bbd9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# grid search method \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [1,5,7,10,15,25,50],\n",
    "              'gamma': [.1,.5,.10,.25,.50,1]}\n",
    "GS = GridSearchCV(SVC(kernel='rbf'),param_grid,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c5743c93-6e00-4d86-b46f-7f2581663655",
    "_uuid": "b18d724b4a80badef44189454b4790f0e88ac4e0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "GS.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d4d4803-2858-4767-bffc-b3692dd04f10",
    "_uuid": "8b9d9add3fb4fc19e57e5726b2933bf23be2f063",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"the parameters {} are the best.\".format(GS.best_params_))\n",
    "print(\"the best score is {:.2f}.\".format(GS.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf90ef41-1cca-4d3c-b544-fa5a2b41b2fd",
    "_uuid": "2f767e48ec82fbd49d3d7d8c925aee489e35711d",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Kernalized SVM machine \n",
    "svr2 = SVR(degree=2,C=100,epsilon=.01).fit(X_train,Y_train)\n",
    "print(\"{:.2f} is the accuracy of the SV Regressor\".format(svr2.score(X_train,Y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "44b3e578-a4a2-48b3-8da6-22c532c2c831",
    "_uuid": "b8b6b3b800dee86d7f26e21cf36c3d57e035b41e"
   },
   "source": [
    "We can notice that the kernalised Support vector machine regressor gives better accuracy than the previous Linear Regressor(non kernal) SVM. Never the less one, needs to understand the data one is work on before trying out various methods. Cross validation techniques are useful.\n",
    "\n",
    "I may futher add Cross Validation techniques for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d04d0cb5-cd1f-4c82-be02-8be365a05016",
    "_uuid": "a058f56b11f09d5249ee05feaf392e160914a262",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test = test.drop(['id'],axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32576733-0a12-4a81-beec-a22358aff6f4",
    "_uuid": "7346ac57853254a4ec52994c84c100e160dc92aa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_mat = np.array(test)\n",
    "test = scaler.fit_transform(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a8b849d-482c-4550-9bbe-7125b3621e23",
    "_uuid": "b007b4b6654c90db0ab84d8b2369d69b50c9752a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf4 = SVC(kernel='rbf',C=25,gamma=.1).fit(X_train,Y_train)\n",
    "prediction = clf4.predict(test_mat)\n",
    "pred = pd.DataFrame(prediction)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c2c43670-f196-4877-b4bb-d50e21c0fc39",
    "_uuid": "93a6d7562c2960d64ffdfe36da3ccbced09b7c3c",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prediction = svr2.predict(test_mat)\n",
    "pred = pd.DataFrame(prediction)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3f7a7ad-b79b-4a2b-9e5c-1d2b695931bc",
    "_uuid": "c55062d7f85989948e7aace59e0b259ad93e5d01"
   },
   "source": [
    "We have predicted the value of the test set that was provided to us in the data set and we can from the previous 2 blocks that our predictions are pretty accurate. Looks Good. !! Enjoy !! \n",
    "Post your comments for Discussio"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
